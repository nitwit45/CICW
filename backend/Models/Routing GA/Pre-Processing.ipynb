{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e3c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ceac7",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa8a0d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_select = ['Order City', 'Customer City', 'Latitude', 'Longitude', 'Department Id', 'Department Name',\n",
    "                     'Order Id', 'Category Name', 'Customer Id', 'Order Item Quantity', 'shipping date (DateOrders)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0323590a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated data (excluding 'Rome' Customer City) written to 'data1.csv'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data = pd.read_csv('DataCoSupplyChainDataset.csv', usecols=columns_to_select, encoding='ISO-8859-1')\n",
    "    same_city_df = data[data['Order City'] == data['Customer City']]\n",
    "    same_city_df = same_city_df[same_city_df['Customer City'] != 'Rome']  # Remove rows where Customer City is 'Rome'\n",
    "\n",
    "    same_city_df.to_csv('data1.csv', index=False)\n",
    "    print(\"Consolidated data (excluding 'Rome' Customer City) written to 'data1.csv'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56850694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order city\n",
      "['Aurora', 'Austin', 'Chicago', 'Columbus', 'Houston', 'Los Angeles', 'Louisville', 'Marion', 'Miami', 'Newark', 'Philadelphia', 'San Antonio', 'San Diego', 'San Francisco', 'Santa Ana', 'Seattle']\n",
      "\n",
      "\n",
      "category name\n",
      "['Accessories', 'Camping & Hiking', 'Cardio Equipment', 'Cleats', 'Fishing', 'Fitness Accessories', 'Golf Apparel', 'Golf Balls', 'Golf Gloves', 'Golf Shoes', 'Hunting & Shooting', 'Indoor/Outdoor Games', \"Men's Footwear\", 'Shop By Sport', 'Water Sports', \"Women's Apparel\"]\n",
      "\n",
      "\n",
      "customer city\n",
      "['Aurora', 'Austin', 'Chicago', 'Columbus', 'Houston', 'Los Angeles', 'Louisville', 'Marion', 'Miami', 'Newark', 'Philadelphia', 'San Antonio', 'San Diego', 'San Francisco', 'Santa Ana', 'Seattle']\n",
      "\n",
      "\n",
      "department id\n",
      "[3, 4, 5, 6, 7]\n",
      "\n",
      "\n",
      "department name\n",
      "['Apparel', 'Fan Shop', 'Footwear', 'Golf', 'Outdoors']\n",
      "\n",
      "\n",
      "date\n",
      "['4/1/2017 11:59', '4/10/2016 10:44', '4/15/2016 20:22', '4/16/2016 14:14', '4/24/2016 18:04', '4/26/2016 17:53', '4/27/2016 9:08', '4/30/2016 22:16', '4/30/2016 3:42', '4/4/2016 21:20', '4/5/2016 12:19', '4/6/2016 9:10', '4/8/2016 8:17', '5/11/2016 13:19', '5/11/2016 22:15', '5/20/2016 21:32', '5/22/2016 5:04', '5/30/2016 3:39', '6/11/2016 20:38', '6/11/2016 5:02', '6/13/2016 3:59', '6/16/2016 11:31', '6/2/2016 2:46', '6/20/2016 11:10', '6/20/2016 2:24', '6/29/2016 14:18', '7/15/2016 9:01', '7/19/2016 21:38', '7/21/2016 2:53', '7/22/2016 22:51', '7/26/2016 13:12', '7/26/2016 7:26', '7/28/2016 1:18', '7/5/2016 15:52', '7/7/2016 11:19', '7/8/2016 2:01', '8/10/2016 8:07', '8/14/2016 18:16', '8/2/2016 21:05', '8/4/2016 19:20', '8/5/2016 3:44', '8/8/2016 10:55']\n",
      "\n",
      "\n",
      "lat\n",
      "[25.76825142, 25.88852501, 29.47656632, 29.71482086, 29.7805748, 29.78513908, 29.88981438, 30.35012245, 32.52082825, 32.71385574, 33.63597488, 33.69957733, 33.93173981, 33.94827271, 33.96860123, 33.98547363, 34.0015831, 34.02769089, 34.03247452, 34.03800583, 34.06108093, 34.08999252, 34.1137352, 34.11491013, 37.71640396, 38.17819977, 39.67436981, 39.70932007, 39.95703888, 40.01562119, 40.7018013, 40.75481033, 41.83272171, 41.89517212, 41.895607, 41.89567947, 41.96846008, 41.98498154, 47.67869949]\n",
      "\n",
      "\n",
      "lon\n",
      "[-122.4407883, -122.3217621, -118.4321518, -118.4061966, -118.3382416, -118.3267975, -118.2774735, -118.2747421, -118.2574081, -118.2431641, -118.2400131, -118.2007599, -118.1886444, -118.1542206, -118.0218811, -117.9125671, -117.1392975, -104.8145828, -98.579216, -97.69873047, -95.51293945, -95.47782135, -95.37239838, -95.17199707, -87.98048401, -87.82719421, -87.73634338, -87.70032501, -87.69122315, -87.65660858, -85.76452637, -84.88657379, -83.07437134, -80.19455719, -80.17294312, -75.76576996, -75.22582245, -75.08410645, -74.1746521]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('data1.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Extract unique values from the specified column\n",
    "unique_values1 = df['Order City'].unique()\n",
    "unique_values2 = df['Category Name'].unique()\n",
    "unique_values3 = df['Customer City'].unique()\n",
    "unique_values4 = df['Department Id'].unique()\n",
    "unique_values5 = df['Department Name'].unique()\n",
    "unique_values6 = df['shipping date (DateOrders)'].unique()\n",
    "unique_values7 = df['Latitude'].unique()\n",
    "unique_values8 = df['Longitude'].unique()\n",
    "\n",
    "# Sort the unique values in ascending order\n",
    "unique_values_sorted1 = sorted(unique_values1)\n",
    "unique_values_sorted2 = sorted(unique_values2)\n",
    "unique_values_sorted3 = sorted(unique_values3)\n",
    "unique_values_sorted4 = sorted(unique_values4)\n",
    "unique_values_sorted5 = sorted(unique_values5)\n",
    "unique_values_sorted6 = sorted(unique_values6)\n",
    "unique_values_sorted7 = sorted(unique_values7)\n",
    "unique_values_sorted8 = sorted(unique_values8)\n",
    "\n",
    "# Print the unique values\n",
    "print('Order city')\n",
    "print(unique_values_sorted1)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('category name')\n",
    "print(unique_values_sorted2)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('customer city')\n",
    "print(unique_values_sorted3)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('department id')\n",
    "print(unique_values_sorted4)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('department name')\n",
    "print(unique_values_sorted5)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('date')\n",
    "print(unique_values_sorted6)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('lat')\n",
    "print(unique_values_sorted7)\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('lon')\n",
    "print(unique_values_sorted8)\n",
    "print('')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5fb5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the existing dataset from CSV\n",
    "existing_data = pd.read_csv('data1.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Extract existing order IDs from the dataset\n",
    "existing_order_ids = existing_data['Order Id'].tolist()\n",
    "\n",
    "# Generating sample data\n",
    "num_rows = 1000  # Number of rows in the synthetic dataset to be generated\n",
    "department_ids = list(range(3, 8))  # Department IDs ranging from 2 to 12\n",
    "\n",
    "# Generating random dates within a range\n",
    "# start_date = datetime(2015, 1, 1)\n",
    "# end_date = datetime(2018, 12, 31)\n",
    "# date_list = [start_date + timedelta(days=random.randint(0, (end_date - start_date).days)) for _ in range(num_rows)]\n",
    "\n",
    "# Sample city, country, and department names\n",
    "# order_cities = ['Santo Domingo', 'New York City', 'Los Angeles', 'Tegucigalpa', 'Managua']\n",
    "cities = ['Aurora', 'Austin', 'Chicago', 'Columbus', 'Houston', 'Los Angeles', 'Louisville', \n",
    "                'Marion', 'Miami', 'Newark', 'Philadelphia', 'San Antonio', 'San Diego', 'San Francisco', 'Santa Ana', 'Seattle']\n",
    "# order_countries = ['Estados Unidos', 'Francia', 'MÃ©xico', 'Alemania', 'Australia']\n",
    "# customer_countries = ['EE. UU.', 'Puerto Rico']\n",
    "# customer_cities = ['Caguas', 'Chicago', 'Los Angeles', 'Brooklyn', 'New York']\n",
    "department_names = ['Apparel', 'Fan Shop', 'Footwear', 'Golf', 'Outdoors']\n",
    "category_names = ['Crafts', 'DVDs', 'CDs', 'Books', 'Garden', 'Music', \n",
    "                  'Camping & Hiking', 'Tennis & Racquet', 'Lacrosse', 'Water Sports', 'Indoor/Outdoor Games',\n",
    "                 'Electronics', 'Cameras', 'Computers', 'Health and Beauty', 'Video Games',\n",
    "                 'Cleats', \"Women's Apparel\", \"Kids' Golf Clubs\", 'Baseball & Softball', 'Soccer', 'Accessories',\n",
    "        \"Girls' Apparel\", \"Women's Clothing\", \"Men's Clothing\", 'Fitness Accessories', 'Golf Balls', 'Golf Gloves',\n",
    "                 'Cardio Equipment', \"Men's Footwear\", 'As Seen on TV!', 'Strength Training', 'Baby', 'Fishing', 'Toys',\n",
    "                 'Basketball', 'Golf Bags & Carts', \"Women's Golf Clubs\", \"Men's Golf Clubs\",\n",
    "                 'Trade-In', 'Hockey', 'Golf Shoes', 'Boxing & MMA', 'Consumer Electronics', 'Pet Supplies',\n",
    "                 'Golf Apparel', 'Hunting & Shooting', 'Golf Carts', 'other']\n",
    "dates = ['4/1/2017 11:59', '4/10/2016 10:44', '4/15/2016 20:22', '4/16/2016 14:14', '4/24/2016 18:04', \n",
    "         '4/26/2016 17:53', '4/27/2016 9:08', '4/30/2016 22:16', '4/30/2016 3:42', '4/4/2016 21:20', \n",
    "         '4/5/2016 12:19', '4/6/2016 9:10', '4/8/2016 8:17', '5/11/2016 13:19', '5/11/2016 22:15', \n",
    "         '5/20/2016 21:32', '5/22/2016 5:04', '5/30/2016 3:39', '6/11/2016 20:38', '6/11/2016 5:02',\n",
    "         '6/13/2016 3:59', '6/16/2016 11:31', '6/2/2016 2:46', '6/20/2016 11:10', '6/20/2016 2:24', \n",
    "         '6/29/2016 14:18', '7/15/2016 9:01', '7/19/2016 21:38', '7/21/2016 2:53', '7/22/2016 22:51', \n",
    "         '7/26/2016 13:12', '7/26/2016 7:26', '7/28/2016 1:18', '7/5/2016 15:52', '7/7/2016 11:19',\n",
    "         '7/8/2016 2:01', '8/10/2016 8:07', '8/14/2016 18:16', '8/2/2016 21:05', '8/4/2016 19:20', \n",
    "         '8/5/2016 3:44', '8/8/2016 10:55']\n",
    "\n",
    "lat = ['25.76825142', '25.88852501', '29.47656632', '29.71482086', '29.7805748', '29.78513908', '29.88981438', \n",
    "       '30.35012245', '32.52082825', '32.71385574', '33.63597488', '33.69957733', '33.93173981', '33.94827271', \n",
    "       '33.96860123', '33.98547363', '34.0015831', '34.02769089', '34.03247452', '34.03800583', '34.06108093',\n",
    "       '34.08999252', '34.1137352', '34.11491013', '37.71640396', '38.17819977', '39.67436981', '39.70932007', \n",
    "       '39.95703888', '40.01562119', '40.7018013', '40.75481033', '41.83272171', '41.89517212', '41.895607', '41.89567947',\n",
    "       '41.96846008', '41.98498154', '47.67869949']\n",
    "\n",
    "lon = ['-122.4407883', '-122.3217621', '-118.4321518', '-118.4061966', '-118.3382416', '-118.3267975',\n",
    "       '-118.2774735', '-118.2747421', '-118.2574081', '-118.2431641', '-118.2400131', '-118.2007599',\n",
    "       '-118.1886444', '-118.1542206', '-118.0218811', '-117.9125671', '-117.1392975', '-104.8145828',\n",
    "       '-98.579216', '-97.69873047', '-95.51293945', '-95.47782135', '-95.37239838', '-95.17199707', \n",
    "       '-87.98048401', '-87.82719421', '-87.73634338',' -87.70032501', '-87.69122315', '-87.65660858',\n",
    "       '-85.76452637', '-84.88657379', '-83.07437134', '-80.19455719', '-80.17294312', '-75.76576996',\n",
    "       '-75.22582245', '-75.08410645', '-74.1746521']\n",
    "\n",
    "# Creating the synthetic dataset\n",
    "data = {\n",
    "    'Department Id': [random.choice(department_ids) for _ in range(num_rows)],\n",
    "    'shipping date (DateOrders)': [random.choice(dates) for _ in range(num_rows)],\n",
    "    'Order Id': [random.randint(1000, 9999) for _ in range(num_rows)],\n",
    "    'Customer City': [random.choice(cities) for _ in range(num_rows)],\n",
    "    'Customer Id': [random.randint(10000, 99999) for _ in range(num_rows)],\n",
    "    'Department Name': [random.choice(department_names) for _ in range(num_rows)],\n",
    "#     'Latitude': [random.uniform(-90, 90) for _ in range(num_rows)],\n",
    "#     'Longitude': [random.uniform(-180, 180) for _ in range(num_rows)],\n",
    "#     'Order City': [random.choice(order_cities) for _ in range(num_rows)],\n",
    "#     'Order Country': [random.choice(order_countries) for _ in range(num_rows)],\n",
    "    'Order Item Quantity': [random.randint(1, 10) for _ in range(num_rows)],\n",
    "    'Category Name': [random.choice(category_names) for _ in range(num_rows)],\n",
    "    'Latitude': [random.choice(lat) for _ in range(num_rows)],\n",
    "    'Longitude': [random.choice(lon) for _ in range(num_rows)]\n",
    "}\n",
    "\n",
    "# Getting unique order IDs that are not present in the existing dataset\n",
    "existing_order_ids_set = set(existing_order_ids)\n",
    "unique_order_ids = []\n",
    "for _ in range(num_rows):\n",
    "    new_order_id = random.randint(1000, 9999)\n",
    "    while new_order_id in existing_order_ids_set:\n",
    "        new_order_id = random.randint(1000, 9999)\n",
    "    unique_order_ids.append(new_order_id)\n",
    "\n",
    "# Add unique order IDs to the synthetic dataset\n",
    "data['Order Id'] = unique_order_ids\n",
    "\n",
    "# Creating the DataFrame for the synthetic dataset\n",
    "synthetic_df = pd.DataFrame(data)\n",
    "\n",
    "# Combining existing data with the synthetic data\n",
    "combined_data = pd.concat([existing_data, synthetic_df], ignore_index=True)\n",
    "\n",
    "# Writing the combined dataset to a new CSV file\n",
    "combined_data.to_csv('data2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdae9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the combined dataset with additional columns\n",
    "combined_data = pd.read_csv('data2.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Convert 'shipping date (DateOrders)' column to datetime format\n",
    "combined_data['shipping date (DateOrders)'] = pd.to_datetime(combined_data['shipping date (DateOrders)'])\n",
    "\n",
    "combined_data['Order Item Quantity'] *= 100\n",
    "\n",
    "# Sort data by 'Department Id' and 'shipping date (DateOrders)'\n",
    "sorted_data = combined_data.sort_values(by=['Department Id', 'shipping date (DateOrders)'])\n",
    "\n",
    "# Write the sorted data to a new CSV file\n",
    "sorted_data.to_csv('data3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd3eed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Category Name Customer City  Customer Id  Department Id Department Name  \\\n",
      "0            Garden    Louisville        28630              3            Golf   \n",
      "1  Cardio Equipment      Columbus         4663              3        Footwear   \n",
      "2  Cardio Equipment      Columbus         4663              3        Footwear   \n",
      "3  Cardio Equipment      Columbus         4663              3        Footwear   \n",
      "4          Lacrosse       Houston        69754              3        Outdoors   \n",
      "\n",
      "    Latitude   Longitude Order City  Order Id  Order Item Quantity  \\\n",
      "0  47.678699 -118.200760        NaN      9838                  700   \n",
      "1  32.520828  -84.886574   Columbus     31411                  100   \n",
      "2  32.520828  -84.886574   Columbus     31411                  400   \n",
      "3  32.520828  -84.886574   Columbus     31411                  500   \n",
      "4  40.701801 -118.154221        NaN      5655                  200   \n",
      "\n",
      "  shipping date (DateOrders)  order_latitude  order_longitude  \n",
      "0        2016-04-04 21:20:00       38.254238       -85.759407  \n",
      "1        2016-04-05 12:19:00       39.962260       -83.000707  \n",
      "2        2016-04-05 12:19:00       39.962260       -83.000707  \n",
      "3        2016-04-05 12:19:00       39.962260       -83.000707  \n",
      "4        2016-04-05 12:19:00       29.758938       -95.367697  \n"
     ]
    }
   ],
   "source": [
    "# Function to get coordinates for a city with caching\n",
    "geocache = {}  # Cache to store coordinates for cities already looked up\n",
    "\n",
    "def get_coordinates(city):\n",
    "    global geocache\n",
    "    if city in geocache:\n",
    "        return geocache[city]\n",
    "    else:\n",
    "        geolocator = Nominatim(user_agent=\"my_geocoder\")\n",
    "        location = geolocator.geocode(city)\n",
    "        if location:\n",
    "            coordinates = location.latitude, location.longitude\n",
    "            geocache[city] = coordinates\n",
    "            return coordinates\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('data3.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Apply get_coordinates function to 'Customer City' column\n",
    "df['order_latitude'] = df['Customer City'].apply(lambda x: get_coordinates(x)[0] if get_coordinates(x) else None)\n",
    "df['order_longitude'] = df['Customer City'].apply(lambda x: get_coordinates(x)[1] if get_coordinates(x) else None)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Write the updated DataFrame to a new CSV file\n",
    "df.to_csv('data4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0df1ef42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of branches:  Department Id\n",
      "3    202\n",
      "4    199\n",
      "5    211\n",
      "6    181\n",
      "7    220\n",
      "dtype: int64\n",
      "total orders:  7    255\n",
      "5    236\n",
      "3    220\n",
      "4    219\n",
      "6    194\n",
      "Name: Department Id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('data4.csv', encoding='ISO-8859-1')  # Replace 'your_data.csv' with your actual file path\n",
    "\n",
    "# Group by 'Department Id' and count unique latitude and longitude combinations\n",
    "unique_coords_count = df.groupby('Department Id').apply(lambda x: x[['Latitude', 'Longitude']].drop_duplicates().shape[0])\n",
    "\n",
    "# Display the counts\n",
    "print('no of branches: ' ,unique_coords_count)\n",
    "\n",
    "department_counts = df['Department Id'].value_counts()\n",
    "\n",
    "# Display the counts for each Department Id\n",
    "print('total orders: ' ,department_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4fe47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
